import gradio as gr
import os
import sys
from config import FINETUNED_JUDGE_MODELS, PROPRIETARY_MODELS
from webui.evaluation import evaluate, evaluate_batch, calibrated_evaluation, calibrated_evaluation_batch, evaluate_batch_with_api, calculate_confidence
import pandas as pd
import json
from modelscope import AutoModelForCausalLM, AutoTokenizer
import gc
import torch
import uuid
import tempfile

def enable_evaluate_button(load_status):
    if "æˆåŠŸ" in load_status.lower():
        return gr.update(interactive=True)
    return gr.update(interactive=False)

def update_model_choices(model_type):
    if model_type == "å¾®è°ƒè£åˆ¤æ¨¡å‹":
        return [
            gr.update(
                choices=list(FINETUNED_JUDGE_MODELS.keys()),
                value=list(FINETUNED_JUDGE_MODELS.keys())[0],
                visible=True
            ),
            gr.update(visible=False)
        ]
    else:
        return [
            gr.update(visible=False),
            gr.update(
                choices=list(PROPRIETARY_MODELS.keys()),
                value=list(PROPRIETARY_MODELS.keys())[0],
                visible=True
            )
        ]

def load_model_based_on_type(model_type, finetuned_model_name, proprietary_model_name, eval_mode, state):
    if not isinstance(state, dict):
        print(f"é”™è¯¯ï¼šstate ä¸æ˜¯å­—å…¸ï¼Œæ”¶åˆ° {type(state)}: {state}")
        return "é”™è¯¯ï¼šå†…éƒ¨çŠ¶æ€é”™è¯¯ï¼Œè¯·åˆ·æ–°é¡µé¢", gr.update(interactive=True)
    try:
        if eval_mode == "å•æ¨¡å‹è¯„ä¼°":
            if model_type == "å¾®è°ƒè£åˆ¤æ¨¡å‹":
                if not finetuned_model_name:
                    return "é”™è¯¯ï¼šæœªé€‰æ‹©å¾®è°ƒæ¨¡å‹", gr.update(interactive=True)
                if finetuned_model_name not in FINETUNED_JUDGE_MODELS:
                    print(f"é”™è¯¯ï¼šæ— æ•ˆçš„å¾®è°ƒæ¨¡å‹ {finetuned_model_name}")
                    return f"é”™è¯¯ï¼šæ— æ•ˆçš„å¾®è°ƒæ¨¡å‹ {finetuned_model_name}", gr.update(interactive=True)
                model_path = FINETUNED_JUDGE_MODELS[finetuned_model_name]
                if not isinstance(model_path, str):
                    return f"é”™è¯¯ï¼šå¾®è°ƒæ¨¡å‹è·¯å¾„å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼Œæ”¶åˆ° {type(model_path)}", gr.update(interactive=True)
                state["finetuned_model_name"] = finetuned_model_name
                state["proprietary_model_name"] = None
                state["model_type"] = model_type
                print(f"Loading model {finetuned_model_name} from {model_path}")
                load_status, button_state = load_model(model_path, state)
                return load_status, button_state
            elif model_type == "ä¸“æœ‰æ¨¡å‹":
                if not proprietary_model_name:
                    return "é”™è¯¯ï¼šæœªé€‰æ‹©ä¸“æœ‰æ¨¡å‹", gr.update(interactive=True)
                if proprietary_model_name not in PROPRIETARY_MODELS:
                    return f"é”™è¯¯ï¼šæ— æ•ˆçš„ä¸“æœ‰æ¨¡å‹ {proprietary_model_name}", gr.update(interactive=True)
                model_path = PROPRIETARY_MODELS[proprietary_model_name]
                if not isinstance(model_path, str):
                    return f"é”™è¯¯ï¼šä¸“æœ‰æ¨¡å‹è·¯å¾„å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼Œæ”¶åˆ° {type(model_path)}", gr.update(interactive=True)
                state["proprietary_model_name"] = proprietary_model_name
                state["finetuned_model_name"] = None
                state["model"] = None
                state["tokenizer"] = None
                state["model_type"] = model_type
                print(f"Initialized proprietary model {proprietary_model_name} with path {model_path}")
                return f"ä¸“æœ‰æ¨¡å‹ {proprietary_model_name} åˆå§‹åŒ–æˆåŠŸï¼", gr.update(interactive=True)
            else:
                return "è¯·é€‰æ‹©æœ‰æ•ˆæ¨¡å‹ç±»å‹", gr.update(interactive=True)
        elif eval_mode == "çº§è”è¯„ä¼°":
            if not finetuned_model_name or not proprietary_model_name:
                return "è¯·é€‰æ‹©å¾®è°ƒè£åˆ¤æ¨¡å‹å’Œä¸“æœ‰æ¨¡å‹", gr.update(interactive=True)
            finetuned_model_path = FINETUNED_JUDGE_MODELS.get(finetuned_model_name)
            if not finetuned_model_path:
                return f"é”™è¯¯ï¼šæ— æ•ˆçš„å¾®è°ƒæ¨¡å‹ {finetuned_model_name}", gr.update(interactive=True)
            if not isinstance(finetuned_model_path, str):
                return f"é”™è¯¯ï¼šå¾®è°ƒæ¨¡å‹è·¯å¾„å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼Œæ”¶åˆ° {type(finetuned_model_path)}", gr.update(interactive=True)
            proprietary_model_path = PROPRIETARY_MODELS.get(proprietary_model_name)
            if not proprietary_model_path:
                return f"é”™è¯¯ï¼šæ— æ•ˆçš„ä¸“æœ‰æ¨¡å‹ {proprietary_model_name}", gr.update(interactive=True)
            if not isinstance(proprietary_model_path, str):
                return f"é”™è¯¯ï¼šä¸“æœ‰æ¨¡å‹è·¯å¾„å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼Œæ”¶åˆ° {type(proprietary_model_path)}", gr.update(interactive=True)
            state["finetuned_model_name"] = finetuned_model_name
            state["proprietary_model_name"] = proprietary_model_name
            state["model_type"] = model_type
            print(f"Loading finetuned model {finetuned_model_name} from {finetuned_model_path}")
            load_status, button_state = load_model(finetuned_model_path, state)
            print(f"Initialized proprietary model {proprietary_model_name} with path {proprietary_model_path}")
            return f"æ¨¡å‹åŠ è½½æˆåŠŸï¼š\n\t- å¾®è°ƒè£åˆ¤æ¨¡å‹: {finetuned_model_name}\n\t- ä¸“æœ‰æ¨¡å‹: {proprietary_model_name}", button_state
        else:
            return "è¯·é€‰æ‹©è¯„ä¼°æ¨¡å¼", gr.update(interactive=True)
    except Exception as e:
        return f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}", gr.update(interactive=True)

def update_model_type(model_type, state):
    if not isinstance(state, dict):
        print(f"é”™è¯¯ï¼šstate ä¸æ˜¯å­—å…¸ï¼Œæ”¶åˆ° {type(state)}: {state}")
        return state
    state["model_type"] = model_type
    return state

def update_calibration_mode(model_type):
    if model_type == "ä¸“æœ‰æ¨¡å‹":
        return gr.update(visible=True, value=False)
    return gr.update(visible=False, value=False)

def manual_evaluate(instruction, answer1, answer2, mode, state, calibration_mode):
    if not isinstance(state, dict):
        return f"é”™è¯¯ï¼šstate ä¸æ˜¯å­—å…¸ï¼Œæ”¶åˆ° {type(state)}", "", gr.update(visible=False)
    finetuned_model_name = state.get("finetuned_model_name")
    model_type = state.get("model_type")
    proprietary_model_name = state.get("proprietary_model_name")
    eval_mode = state.get("eval_mode")
    if eval_mode == "çº§è”è¯„ä¼°":
        llm = state.get("model")
        tokenizer = state.get("tokenizer")
        if llm is None or tokenizer is None:
            return "è¯·å…ˆåŠ è½½å¾®è°ƒæ¨¡å‹", "", gr.update(visible=False)
        verdict, details, logprobs, _, _ = evaluate(instruction, answer1, answer2, mode, state, finetuned_model_name)
        confidence = calculate_confidence(logprobs)
        threshold = state.get("confidence_threshold", 0.5)
        if confidence < threshold:
            if proprietary_model_name:
                if calibration_mode:
                    proprietary_verdict, proprietary_details = calibrated_evaluation(instruction, answer1, answer2, mode, model_name=proprietary_model_name)
                else:
                    proprietary_verdict, proprietary_details, _, _, _ = evaluate(instruction, answer1, answer2, mode, state=state, proprietary_model=proprietary_model_name)
                details = (
                    "<div class='details-section'>"
                    "<h3>çº§è”è¯„ä¼°è¯¦æƒ…</h3>"
                    f"<p>ç½®ä¿¡åº¦ä½äºé˜ˆå€¼ ({confidence:.4f} < {threshold:.4f})ï¼Œå·²è°ƒç”¨ä¸“æœ‰æ¨¡å‹é‡æ–°è¯„ä¼°ã€‚</p>"
                    f"{details}"
                    "<h3>â€ğŸ§‘â€âš–ï¸ ä¸“æœ‰æ¨¡å‹è¯„ä¼°ç»“æœ</h3>"
                    f"{proprietary_details}"
                    "</div>"
                )
                verdict = proprietary_verdict
            else:
                details = (
                    "<div class='details-section'>"
                    f"{details}"
                    f"<p>ç½®ä¿¡åº¦ä½äºé˜ˆå€¼ ({confidence:.4f} < {threshold:.4f})ï¼Œä½†æœªåŠ è½½ä¸“æœ‰æ¨¡å‹ã€‚</p>"
                    "</div>"
                )
        else:
            details = (
                "<div class='details-section'>"
                f"{details}"
                f"<p>ç½®ä¿¡åº¦: {confidence:.4f} (é«˜äºé˜ˆå€¼ {threshold:.4f})</p>"
                "</div>"
            )
        return verdict, details, gr.update(visible=True)
    else:
        if model_type == "ä¸“æœ‰æ¨¡å‹":
            if not proprietary_model_name:
                return "è¯·å…ˆåŠ è½½æ¨¡å‹", "", gr.update(visible=False)
            if calibration_mode:
                verdict, details = calibrated_evaluation(instruction, answer1, answer2, mode, model_name=proprietary_model_name)
                return verdict, details, gr.update(visible=True)
            verdict, details, _, _, _ = evaluate(instruction, answer1, answer2, mode, state=state, proprietary_model=proprietary_model_name)
            return verdict, details, gr.update(visible=True)
        llm = state.get("model")
        tokenizer = state.get("tokenizer")
        if llm is None or tokenizer is None:
            return "è¯·å…ˆåŠ è½½æ¨¡å‹", "", gr.update(visible=False)
        verdict, details, _, _, _ = evaluate(instruction, answer1, answer2, mode, state=state, model_name=finetuned_model_name)
        return verdict, details, gr.update(visible=True)

def update_batch_calibration_mode(model_type):
    if model_type == "ä¸“æœ‰æ¨¡å‹":
        return gr.update(visible=True, value=False)
    return gr.update(visible=False, value=False)

def batch_evaluation(file, mode, state, calibration_mode):
    if not isinstance(state, dict):
        return f"é”™è¯¯ï¼šstate ä¸æ˜¯å­—å…¸ï¼Œæ”¶åˆ° {type(state)}", None
    eval_mode = state.get("eval_mode")
    if eval_mode == "çº§è”è¯„ä¼°":
        llm = state.get("model")
        tokenizer = state.get("tokenizer")
        threshold = state.get("confidence_threshold", 0.5)
        if llm is None or tokenizer is None:
            return "è¯·å…ˆåŠ è½½å¾®è°ƒè£åˆ¤æ¨¡å‹", None
        proprietary_model_name = state.get("proprietary_model_name")
        if not proprietary_model_name:
            return "è¯·å…ˆåŠ è½½ä¸“æœ‰æ¨¡å‹", None
        try:
            df = pd.read_csv(file.name) if file.name.endswith('.csv') else pd.DataFrame(json.load(open(file.name)))
        except Exception as e:
            return f"æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}", None
        results = []
        temp_dir = tempfile.gettempdir()
        output_filename = f"eval_report_{uuid.uuid4().hex[:8]}.csv"
        output_path = os.path.join(temp_dir, output_filename)
        for _, row in df.iterrows():
            instruction = row.get('instruction', '')
            answer1 = row.get('answer1', '')
            answer2 = row.get('answer2', '')
            if not all([instruction, answer1, answer2]):
                results.append("æ•°æ®ä¸å®Œæ•´")
                continue
            verdict, details, logprobs = evaluate(
                instruction, answer1, answer2, 
                mode, state, state.get("finetuned_model_name")
            )
            confidence = calculate_confidence(logprobs)
            if confidence < threshold:
                if calibration_mode:
                    proprietary_verdict, proprietary_details = calibrated_evaluation(
                        instruction, answer1, answer2, 
                        mode, model_name=proprietary_model_name
                    )
                else:
                    proprietary_verdict, proprietary_details, _ = evaluate(
                        instruction, answer1, answer2,
                        mode, state=state, 
                        proprietary_model=proprietary_model_name
                    )
                verdict = proprietary_verdict
                details = (
                    "<div class='details-section'>"
                    f"{details}"
                    f"<p>ç½®ä¿¡åº¦ä½äºé˜ˆå€¼ ({confidence:.4f} < {threshold:.4f})ï¼Œå·²ä½¿ç”¨ä¸“æœ‰æ¨¡å‹é‡æ–°è¯„ä¼°</p>"
                    f"{proprietary_details}"
                    "</div>"
                )
            else:
                details = (
                    "<div class='details-section'>"
                    f"{details}"
                    f"<p>ç½®ä¿¡åº¦: {confidence:.4f} (é«˜äºé˜ˆå€¼ {threshold:.4f})</p>"
                    "</div>"
                )
            results.append(verdict)
        output_df = pd.DataFrame({
            'æŒ‡ä»¤': df.get('instruction', []),
            'ç­”æ¡ˆ 1': df.get('answer1', []),
            'ç­”æ¡ˆ 2': df.get('answer2', []),
            'è¯„ä¼°ç»“æœ': results
        })
        try:
            output_df.to_csv(output_path, index=False, encoding='utf-8')
            return f"è¯„ä¼°å®Œæˆï¼Œç‚¹å‡»ä¸‹æ–¹ä¸‹è½½æŠ¥å‘Š", output_path
        except Exception as e:
            return f"ä¿å­˜ç»“æœå¤±è´¥ï¼š{str(e)}", None
    else:
        model_type = state.get("model_type")
        if model_type == "ä¸“æœ‰æ¨¡å‹":
            model_name = state.get("proprietary_model_name")
            if not model_name:
                return "è¯·å…ˆåŠ è½½ä¸“æœ‰æ¨¡å‹", None
            if calibration_mode:
                return calibrated_evaluation_batch(file, mode, model_name=model_name)
            return evaluate_batch(file, mode, state)
        llm = state.get("model")
        tokenizer = state.get("tokenizer")
        if llm is None or tokenizer is None:
            return "è¯·å…ˆåŠ è½½æ¨¡å‹", None
        if calibration_mode:
            return "æ ¡å‡†æ¨¡å¼åªèƒ½ç”¨äºä¸“æœ‰æ¨¡å‹", None
        return evaluate_batch(file, mode, state)

def update_eval_mode(mode, state):
    if not isinstance(state, dict):
        print(f"é”™è¯¯ï¼šstate ä¸æ˜¯å­—å…¸ï¼Œæ”¶åˆ° {type(state)}: {state}")
        return [
            gr.update(visible=False),
            gr.update(visible=False),
            gr.update(visible=False),
            gr.update(choices=["å¾®è°ƒè£åˆ¤æ¨¡å‹"], value="å¾®è°ƒè£åˆ¤æ¨¡å‹"),
            gr.update(visible=True),
            gr.update(visible=False),
            gr.update(visible=True),
            gr.update(visible=False),
        ]
    state["eval_mode"] = mode
    return [
        gr.update(visible=mode == "å•æ¨¡å‹è¯„ä¼°"),
        gr.update(visible=mode == "çº§è”è¯„ä¼°"),
        gr.update(visible=mode == "çº§è”è¯„ä¼°"),
        gr.update(choices=["å¾®è°ƒè£åˆ¤æ¨¡å‹", "ä¸“æœ‰æ¨¡å‹"] if mode == "å•æ¨¡å‹è¯„ä¼°" else ["å¾®è°ƒè£åˆ¤æ¨¡å‹"], value="å¾®è°ƒè£åˆ¤æ¨¡å‹"),
        gr.update(visible=True),
        gr.update(visible=mode == "çº§è”è¯„ä¼°" or mode == "å•æ¨¡å‹è¯„ä¼°"),
        gr.update(visible=True),
        gr.update(visible=mode == "çº§è”è¯„ä¼°" or mode == "å•æ¨¡å‹è¯„ä¼°"),
    ]

def load_model(model_path, state):
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        ).to(device)
        state["model"] = model
        state["tokenizer"] = tokenizer
        return "æ¨¡å‹åŠ è½½æˆåŠŸï¼", gr.update(interactive=True)
    except RuntimeError as re:
        print(f"RuntimeError during model loading: {re}")
        if "out of memory" in str(re).lower():
            return "æ¨¡å‹åŠ è½½å¤±è´¥ï¼šå†…å­˜ä¸è¶³ã€‚è¯·é‡Šæ”¾å†…å­˜æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹ã€‚", gr.update(interactive=True)
        return f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{re}", gr.update(interactive=True)
    except ValueError as ve:
        print(f"ValueError during model loading: {ve}")
        if "is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'" in str(ve):
            return f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼šè¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„ '{model_path}' æ˜¯å¦æ­£ç¡®ï¼Œæˆ–æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½ã€‚", gr.update(interactive=True)
        return f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{ve}", gr.update(interactive=True)
    except OSError as oe:
        print(f"OSError during model loading: {oe}")
        if "No such file or directory" in str(oe):
            return f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼šè·¯å¾„ '{model_path}' ä¸å­˜åœ¨ã€‚è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚", gr.update(interactive=True)
        return f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{oe}", gr.update(interactive=True)
    except Exception as e:
        return f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼šæœªçŸ¥é”™è¯¯ï¼š{e}", gr.update(interactive=True)
    finally:
        gc.collect()

def clear_model(state):
    model_names = []
    if state.get("finetuned_model_name"):
        model_names.append(state.get("finetuned_model_name"))
    if state.get("proprietary_model_name"):
        model_names.append(state.get("proprietary_model_name"))
    
    if state.get("model"):
        del state["model"]
        state["model"] = None
    if state.get("tokenizer"):
        del state["tokenizer"]
        state["tokenizer"] = None
    
    if model_names:
        torch.cuda.empty_cache()
        gc.collect()
    
    state["finetuned_model_name"] = None
    state["proprietary_model_name"] = None
    
    if not model_names:
        return "æ²¡æœ‰æ¨¡å‹éœ€è¦å¸è½½"
    if len(model_names) == 1:
        return "æ¨¡å‹ '{}' å·²å¸è½½".format(model_names[0])
    return "æ¨¡å‹ {} å·²å¸è½½".format(", ".join("'{}'".format(name) for name in model_names))