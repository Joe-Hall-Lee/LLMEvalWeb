import gradio as gr
import os
import sys
from config import FINETUNED_JUDGE_MODELS, PROPRIETARY_MODELS
from webui.evaluation import evaluate, evaluate_batch, toggle_details, calibrated_evaluation, calibrated_evaluation_batch, evaluate_batch_with_api, calculate_confidence
from models.model import load_model, clear_model
import pandas as pd
import json

def enable_evaluate_button(load_status):
    if "æˆåŠŸ" in load_status.lower():
        return gr.update(interactive=True)
    return gr.update(interactive=False)

def update_model_choices(model_type):
    if model_type == "å¾®è°ƒè£åˆ¤æ¨¡å‹":
        return [
            gr.update(
                choices=list(FINETUNED_JUDGE_MODELS.keys()),
                value=list(FINETUNED_JUDGE_MODELS.keys())[0],
                visible=True
            ),
            gr.update(visible=False)
        ]
    else:
        return [
            gr.update(visible=False),
            gr.update(
                choices=list(PROPRIETARY_MODELS.keys()),
                value=list(PROPRIETARY_MODELS.keys())[0],
                visible=True
            )
        ]

def load_model_based_on_type(model_type, finetuned_model_name, proprietary_model_name, eval_mode, state):
    if not isinstance(state, dict):
        print(f"é”™è¯¯ï¼šstate ä¸æ˜¯å­—å…¸ï¼Œæ”¶åˆ° {type(state)}: {state}")
        return "é”™è¯¯ï¼šå†…éƒ¨çŠ¶æ€é”™è¯¯ï¼Œè¯·åˆ·æ–°é¡µé¢", gr.update(interactive=True)
    try:
        if eval_mode == "å•æ¨¡å‹è¯„ä¼°":
            if model_type == "å¾®è°ƒè£åˆ¤æ¨¡å‹":
                if not finetuned_model_name:
                    return "é”™è¯¯ï¼šæœªé€‰æ‹©å¾®è°ƒæ¨¡å‹", gr.update(interactive=True)
                if finetuned_model_name not in FINETUNED_JUDGE_MODELS:
                    return f"é”™è¯¯ï¼šæ— æ•ˆçš„å¾®è°ƒæ¨¡å‹ {finetuned_model_name}", gr.update(interactive=True)
                model_path = FINETUNED_JUDGE_MODELS[finetuned_model_name]
                if not isinstance(model_path, str):
                    return f"é”™è¯¯ï¼šå¾®è°ƒæ¨¡å‹è·¯å¾„å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼Œæ”¶åˆ° {type(model_path)}", gr.update(interactive=True)
                state["finetuned_model_name"] = finetuned_model_name
                state["proprietary_model_name"] = None
                state["model_type"] = model_type
                print(f"Loading model {finetuned_model_name} from {model_path}")
                load_status, button_state = load_model(model_path, state)
                return load_status, button_state
            elif model_type == "ä¸“æœ‰æ¨¡å‹":
                if not proprietary_model_name:
                    return "é”™è¯¯ï¼šæœªé€‰æ‹©ä¸“æœ‰æ¨¡å‹", gr.update(interactive=True)
                if proprietary_model_name not in PROPRIETARY_MODELS:
                    return f"é”™è¯¯ï¼šæ— æ•ˆçš„ä¸“æœ‰æ¨¡å‹ {proprietary_model_name}", gr.update(interactive=True)
                model_path = PROPRIETARY_MODELS[proprietary_model_name]
                if not isinstance(model_path, str):
                    return f"é”™è¯¯ï¼šä¸“æœ‰æ¨¡å‹è·¯å¾„å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼Œæ”¶åˆ° {type(model_path)}", gr.update(interactive=True)
                state["proprietary_model_name"] = proprietary_model_name
                state["finetuned_model_name"] = None
                state["model"] = None
                state["tokenizer"] = None
                state["model_type"] = model_type
                print(f"Initialized proprietary model {proprietary_model_name} with path {model_path}")
                return f"ä¸“æœ‰æ¨¡å‹ {proprietary_model_name} åˆå§‹åŒ–æˆåŠŸï¼", gr.update(interactive=True)
            else:
                return "è¯·é€‰æ‹©æœ‰æ•ˆæ¨¡å‹ç±»å‹", gr.update(interactive=True)
        elif eval_mode == "çº§è”è¯„ä¼°":
            if not finetuned_model_name or not proprietary_model_name:
                return "è¯·é€‰æ‹©å¾®è°ƒè£åˆ¤æ¨¡å‹å’Œä¸“æœ‰æ¨¡å‹", gr.update(interactive=True)
            finetuned_model_path = FINETUNED_JUDGE_MODELS.get(finetuned_model_name)
            if not finetuned_model_path:
                return f"é”™è¯¯ï¼šæ— æ•ˆçš„å¾®è°ƒæ¨¡å‹ {finetuned_model_name}", gr.update(interactive=True)
            if not isinstance(finetuned_model_path, str):
                return f"é”™è¯¯ï¼šå¾®è°ƒæ¨¡å‹è·¯å¾„å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼Œæ”¶åˆ° {type(finetuned_model_path)}", gr.update(interactive=True)
            proprietary_model_path = PROPRIETARY_MODELS.get(proprietary_model_name)
            if not proprietary_model_path:
                return f"é”™è¯¯ï¼šæ— æ•ˆçš„ä¸“æœ‰æ¨¡å‹ {proprietary_model_name}", gr.update(interactive=True)
            if not isinstance(proprietary_model_path, str):
                return f"é”™è¯¯ï¼šä¸“æœ‰æ¨¡å‹è·¯å¾„å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼Œæ”¶åˆ° {type(proprietary_model_path)}", gr.update(interactive=True)
            state["finetuned_model_name"] = finetuned_model_name
            state["proprietary_model_name"] = proprietary_model_name
            state["model_type"] = model_type
            print(f"Loading finetuned model {finetuned_model_name} from {finetuned_model_path}")
            load_status, button_state = load_model(finetuned_model_path, state)
            print(f"Initialized proprietary model {proprietary_model_name} with path {proprietary_model_path}")
            return f"æ¨¡å‹åŠ è½½æˆåŠŸï¼š\n\t- å¾®è°ƒè£åˆ¤æ¨¡å‹: {finetuned_model_name}\n\t- ä¸“æœ‰æ¨¡å‹: {proprietary_model_name}", button_state
        else:
            return "è¯·é€‰æ‹©è¯„ä¼°æ¨¡å¼", gr.update(interactive=True)
    except Exception as e:
        return f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}", gr.update(interactive=True)

def update_model_type(model_type, state):
    if not isinstance(state, dict):
        print(f"é”™è¯¯ï¼šstate ä¸æ˜¯å­—å…¸ï¼Œæ”¶åˆ° {type(state)}: {state}")
        return state
    state["model_type"] = model_type
    return state

def update_calibration_mode(model_type):
    if model_type == "ä¸“æœ‰æ¨¡å‹":
        return gr.update(visible=True, value=False)
    return gr.update(visible=False, value=False)

def show_calibration_mode(model_type):
    return gr.update(visible=model_type == "ä¸“æœ‰æ¨¡å‹")

def manual_evaluate(instruction, answer1, answer2, mode, state, calibration_mode):
    if not isinstance(state, dict):
        return f"é”™è¯¯ï¼šstate ä¸æ˜¯å­—å…¸ï¼Œæ”¶åˆ° {type(state)}", "", gr.update(visible=False)
    finetuned_model_name = state.get("finetuned_model_name")
    model_type = state.get("model_type")
    proprietary_model_name = state.get("proprietary_model_name")
    eval_mode = state.get("eval_mode")
    if eval_mode == "çº§è”è¯„ä¼°":
        llm = state.get("model")
        tokenizer = state.get("tokenizer")
        if llm is None or tokenizer is None:
            return "è¯·å…ˆåŠ è½½å¾®è°ƒæ¨¡å‹", "", gr.update(visible=False)
        verdict, details, logprobs = evaluate(instruction, answer1, answer2, mode, state, finetuned_model_name)
        confidence = calculate_confidence(logprobs)
        threshold = state.get("confidence_threshold", 0.5)
        if confidence < threshold:
            if proprietary_model_name:
                if calibration_mode:
                    proprietary_verdict, proprietary_details = calibrated_evaluation(instruction, answer1, answer2, mode, model_name=proprietary_model_name)
                else:
                    proprietary_verdict, proprietary_details, _ = evaluate(instruction, answer1, answer2, mode, state=state, proprietary_model=proprietary_model_name)
                details += f"""
<p>ç½®ä¿¡åº¦ä½äºé˜ˆå€¼ ({confidence:.4f} < {threshold:.4f})ï¼Œå·²è°ƒç”¨ä¸“æœ‰æ¨¡å‹é‡æ–°è¯„ä¼°ã€‚</p>
<h3>â€ğŸ§‘â€âš–ï¸ ä¸“æœ‰æ¨¡å‹è¯„ä¼°ç»“æœ</h3>
{proprietary_details}
"""
                verdict = proprietary_verdict
            else:
                details += f"\n<p>ç½®ä¿¡åº¦ä½äºé˜ˆå€¼ ({confidence:.4f} < {threshold:.4f})ï¼Œä½†æœªåŠ è½½ä¸“æœ‰æ¨¡å‹ã€‚</p>"
        else:
            details += f"\n<p>ç½®ä¿¡åº¦: {confidence:.4f} (é«˜äºé˜ˆå€¼ {threshold:.4f})</p>"
        return verdict, details, gr.update(visible=True)
    else:
        if model_type == "ä¸“æœ‰æ¨¡å‹":
            if not proprietary_model_name:
                return "è¯·å…ˆåŠ è½½æ¨¡å‹", "", gr.update(visible=False)
            if calibration_mode:
                return calibrated_evaluation(instruction, answer1, answer2, mode, model_name=proprietary_model_name)
            return evaluate(instruction, answer1, answer2, mode, state=state, proprietary_model=proprietary_model_name)
        llm = state.get("model")
        tokenizer = state.get("tokenizer")
        if llm is None or tokenizer is None:
            return "è¯·å…ˆåŠ è½½æ¨¡å‹", "", gr.update(visible=False)
        return evaluate(instruction, answer1, answer2, mode, state=state, model_name=finetuned_model_name)

def show_batch_calibration_mode(model_type):
    return gr.update(visible=model_type == "ä¸“æœ‰æ¨¡å‹")

def update_batch_calibration_mode(model_type):
    if model_type == "ä¸“æœ‰æ¨¡å‹":
        return gr.update(visible=True, value=False)
    return gr.update(visible=False, value=False)

def batch_evaluation(file, path, mode, state, calibration_mode):
    if not isinstance(state, dict):
        return f"é”™è¯¯ï¼šstate ä¸æ˜¯å­—å…¸ï¼Œæ”¶åˆ° {type(state)}"
    eval_mode = state.get("eval_mode")
    if eval_mode == "çº§è”è¯„ä¼°":
        llm = state.get("model")
        tokenizer = state.get("tokenizer")
        threshold = state.get("confidence_threshold", 0.5)
        if llm is None or tokenizer is None:
            return "è¯·å…ˆåŠ è½½å¾®è°ƒè£åˆ¤æ¨¡å‹"
        proprietary_model_name = state.get("proprietary_model_name")
        if not proprietary_model_name:
            return "è¯·å…ˆåŠ è½½ä¸“æœ‰æ¨¡å‹"
        try:
            df = pd.read_csv(file.name) if file.name.endswith('.csv') else pd.DataFrame(json.load(open(file.name)))
        except Exception as e:
            return f"æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}"
        results = []
        details_list = []
        for _, row in df.iterrows():
            instruction = row.get('instruction', '')
            answer1 = row.get('answer1', '')
            answer2 = row.get('answer2', '')
            if not all([instruction, answer1, answer2]):
                results.append("æ•°æ®ä¸å®Œæ•´")
                continue
            verdict, details, logprobs = evaluate(
                instruction, answer1, answer2, 
                mode, state, state.get("finetuned_model_name")
            )
            confidence = calculate_confidence(logprobs)
            if confidence < threshold:
                if calibration_mode:
                    proprietary_verdict, proprietary_details = calibrated_evaluation(
                        instruction, answer1, answer2, 
                        mode, model_name=proprietary_model_name
                    )
                else:
                    proprietary_verdict, proprietary_details, _ = evaluate(
                        instruction, answer1, answer2,
                        mode, state=state, 
                        proprietary_model=proprietary_model_name
                    )
                verdict = proprietary_verdict
                details += f"\nç½®ä¿¡åº¦ä½äºé˜ˆå€¼ ({confidence:.4f} < {threshold:.4f})ï¼Œå·²ä½¿ç”¨ä¸“æœ‰æ¨¡å‹é‡æ–°è¯„ä¼°"
            else:
                details += f"\nç½®ä¿¡åº¦: {confidence:.4f} (é«˜äºé˜ˆå€¼ {threshold:.4f})"
            results.append(verdict)
            details_list.append(details)
        output_df = pd.DataFrame({
            'æŒ‡ä»¤': df.get('instruction', []),
            'ç­”æ¡ˆ 1': df.get('answer1', []),
            'ç­”æ¡ˆ 2': df.get('answer2', []),
            'è¯„ä¼°ç»“æœ': results
        })
        try:
            output_df.to_csv(path, index=False, encoding='utf-8')
            return f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ° {path}"
        except Exception as e:
            return f"ä¿å­˜ç»“æœå¤±è´¥ï¼š{str(e)}"
    else:
        model_type = state.get("model_type")
        if model_type == "ä¸“æœ‰æ¨¡å‹":
            model_name = state.get("proprietary_model_name")
            if not model_name:
                return "è¯·å…ˆåŠ è½½ä¸“æœ‰æ¨¡å‹"
            if calibration_mode:
                return calibrated_evaluation_batch(file, path, mode, model_name=model_name)
            return evaluate_batch_with_api(file, path, mode, model_name)
        llm = state.get("model")
        tokenizer = state.get("tokenizer")
        if llm is None or tokenizer is None:
            return "è¯·å…ˆåŠ è½½æ¨¡å‹"
        if calibration_mode:
            return "æ ¡å‡†æ¨¡å¼åªèƒ½ç”¨äºä¸“æœ‰æ¨¡å‹"
        return evaluate_batch(file, path, mode, state)

def update_eval_mode(mode, state):
    if not isinstance(state, dict):
        print(f"é”™è¯¯ï¼šstate ä¸æ˜¯å­—å…¸ï¼Œæ”¶åˆ° {type(state)}: {state}")
        return [
            gr.update(visible=False),
            gr.update(visible=False),
            gr.update(visible=False),
            gr.update(choices=["å¾®è°ƒè£åˆ¤æ¨¡å‹"], value="å¾®è°ƒè£åˆ¤æ¨¡å‹"),
            gr.update(visible=True),
            gr.update(visible=False),
            gr.update(visible=True),
            gr.update(visible=False),
        ]
    state["eval_mode"] = mode
    return [
        gr.update(visible=mode == "å•æ¨¡å‹è¯„ä¼°"),
        gr.update(visible=mode == "çº§è”è¯„ä¼°"),
        gr.update(visible=mode == "çº§è”è¯„ä¼°"),
        gr.update(choices=["å¾®è°ƒè£åˆ¤æ¨¡å‹", "ä¸“æœ‰æ¨¡å‹"] if mode == "å•æ¨¡å‹è¯„ä¼°" else ["å¾®è°ƒè£åˆ¤æ¨¡å‹"], value="å¾®è°ƒè£åˆ¤æ¨¡å‹"),
        gr.update(visible=True),
        gr.update(visible=mode == "çº§è”è¯„ä¼°" or mode == "å•æ¨¡å‹è¯„ä¼°"),
        gr.update(visible=True),
        gr.update(visible=mode == "çº§è”è¯„ä¼°" or mode == "å•æ¨¡å‹è¯„ä¼°"),
    ]